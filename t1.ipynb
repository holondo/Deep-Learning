{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guilherme Holanda Sanches - 10734370\n",
    "# Projeto 1 - Neural Networks\n",
    "---\n",
    "### Questão 1:\n",
    "* A) Observando os dados da base de treino fornecida, é possível notar um grande desbalanceamento entre as classes no que se refere à quantidade de exemplos fornecidos. Portanto a métrica AUC (Area Under the Curve) é mais adequada para avaliar o desempenho do sistema em comparação com a acurácia. Isso ocorre porque a AUC leva em consideração tanto a taxa de verdadeiros positivos quanto a taxa de falsos positivos, fornecendo uma medida mais abrangente da capacidade do modelo em classificar corretamente as amostras de ambas as classes, mesmo quando a distribuição dos dados é desigual."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* B) - (Lendo os dados e gerando \"respostas\" todas negativas e positivas no mesmo tamanho das respostas do test.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.439952</td>\n",
       "      <td>0.683758</td>\n",
       "      <td>1.225814</td>\n",
       "      <td>0.639113</td>\n",
       "      <td>0.716765</td>\n",
       "      <td>0.089295</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>-0.472679</td>\n",
       "      <td>0.352698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211867</td>\n",
       "      <td>0.127443</td>\n",
       "      <td>0.588590</td>\n",
       "      <td>-0.145728</td>\n",
       "      <td>-0.322900</td>\n",
       "      <td>-0.299744</td>\n",
       "      <td>-0.310295</td>\n",
       "      <td>0.248317</td>\n",
       "      <td>-0.011493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.868108</td>\n",
       "      <td>1.264420</td>\n",
       "      <td>-5.167885</td>\n",
       "      <td>3.193648</td>\n",
       "      <td>-3.045621</td>\n",
       "      <td>-2.096166</td>\n",
       "      <td>-6.445610</td>\n",
       "      <td>2.422536</td>\n",
       "      <td>-3.214055</td>\n",
       "      <td>-8.745973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667310</td>\n",
       "      <td>1.269205</td>\n",
       "      <td>0.057657</td>\n",
       "      <td>0.629307</td>\n",
       "      <td>-0.168432</td>\n",
       "      <td>0.443744</td>\n",
       "      <td>0.276539</td>\n",
       "      <td>1.441274</td>\n",
       "      <td>-0.127944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.013114</td>\n",
       "      <td>-0.334412</td>\n",
       "      <td>1.305208</td>\n",
       "      <td>0.837406</td>\n",
       "      <td>-1.126833</td>\n",
       "      <td>-0.064321</td>\n",
       "      <td>-0.594753</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>0.536360</td>\n",
       "      <td>-0.120472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069161</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.112210</td>\n",
       "      <td>-0.016084</td>\n",
       "      <td>0.595033</td>\n",
       "      <td>0.201073</td>\n",
       "      <td>0.278215</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.030762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.969231</td>\n",
       "      <td>-0.233554</td>\n",
       "      <td>0.238473</td>\n",
       "      <td>0.145793</td>\n",
       "      <td>-0.545741</td>\n",
       "      <td>-0.970680</td>\n",
       "      <td>0.347393</td>\n",
       "      <td>-0.209522</td>\n",
       "      <td>-0.342571</td>\n",
       "      <td>-0.100331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240978</td>\n",
       "      <td>-0.362820</td>\n",
       "      <td>-1.417272</td>\n",
       "      <td>0.162136</td>\n",
       "      <td>0.541628</td>\n",
       "      <td>-0.079465</td>\n",
       "      <td>0.268702</td>\n",
       "      <td>-0.101237</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.856523</td>\n",
       "      <td>1.080875</td>\n",
       "      <td>1.866956</td>\n",
       "      <td>1.729941</td>\n",
       "      <td>-0.161741</td>\n",
       "      <td>0.028789</td>\n",
       "      <td>0.401787</td>\n",
       "      <td>0.043774</td>\n",
       "      <td>-0.213916</td>\n",
       "      <td>0.155907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068915</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.077392</td>\n",
       "      <td>-0.221906</td>\n",
       "      <td>0.394141</td>\n",
       "      <td>0.237225</td>\n",
       "      <td>-0.080102</td>\n",
       "      <td>-0.291408</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142398</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142399</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142400</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142401</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142402</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142403 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -0.439952   0.683758  1.225814  0.639113  0.716765  0.089295   \n",
       "1       -4.868108   1.264420 -5.167885  3.193648 -3.045621 -2.096166   \n",
       "2        1.013114  -0.334412  1.305208  0.837406 -1.126833 -0.064321   \n",
       "3        0.969231  -0.233554  0.238473  0.145793 -0.545741 -0.970680   \n",
       "4       -0.856523   1.080875  1.866956  1.729941 -0.161741  0.028789   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "142398 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "142399  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "142400   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "142401  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "142402  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V20       V21  \\\n",
       "0       0.657718  0.034213 -0.472679  0.352698  ...  0.211867  0.127443   \n",
       "1      -6.445610  2.422536 -3.214055 -8.745973  ...  0.667310  1.269205   \n",
       "2      -0.594753  0.147737  0.536360 -0.120472  ...  0.069161  0.017079   \n",
       "3       0.347393 -0.209522 -0.342571 -0.100331  ...  0.240978 -0.362820   \n",
       "4       0.401787  0.043774 -0.213916  0.155907  ... -0.068915  0.007365   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "142398 -4.918215  7.305334  1.914428  4.356170  ...  1.475829  0.213454   \n",
       "142399  0.024330  0.294869  0.584800 -0.975926  ...  0.059616  0.214205   \n",
       "142400 -0.296827  0.708417  0.432454 -0.484782  ...  0.001396  0.232045   \n",
       "142401 -0.686180  0.679145  0.392087 -0.399126  ...  0.127434  0.265245   \n",
       "142402  1.577006 -0.414650  0.486180 -0.915427  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.588590 -0.145728 -0.322900 -0.299744 -0.310295  0.248317 -0.011493   \n",
       "1       0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274 -0.127944   \n",
       "2       0.112210 -0.016084  0.595033  0.201073  0.278215  0.007457  0.030762   \n",
       "3      -1.417272  0.162136  0.541628 -0.079465  0.268702 -0.101237  0.028234   \n",
       "4       0.077392 -0.221906  0.394141  0.237225 -0.080102 -0.291408  0.092140   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "142398  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "142399  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "142400  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "142401  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "142402  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "142398      0  \n",
       "142399      0  \n",
       "142400      0  \n",
       "142401      0  \n",
       "142402      0  \n",
       "\n",
       "[142403 rows x 29 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('test.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.439952</td>\n",
       "      <td>0.683758</td>\n",
       "      <td>1.225814</td>\n",
       "      <td>0.639113</td>\n",
       "      <td>0.716765</td>\n",
       "      <td>0.089295</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>-0.472679</td>\n",
       "      <td>0.352698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211867</td>\n",
       "      <td>0.127443</td>\n",
       "      <td>0.588590</td>\n",
       "      <td>-0.145728</td>\n",
       "      <td>-0.322900</td>\n",
       "      <td>-0.299744</td>\n",
       "      <td>-0.310295</td>\n",
       "      <td>0.248317</td>\n",
       "      <td>-0.011493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.868108</td>\n",
       "      <td>1.264420</td>\n",
       "      <td>-5.167885</td>\n",
       "      <td>3.193648</td>\n",
       "      <td>-3.045621</td>\n",
       "      <td>-2.096166</td>\n",
       "      <td>-6.445610</td>\n",
       "      <td>2.422536</td>\n",
       "      <td>-3.214055</td>\n",
       "      <td>-8.745973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667310</td>\n",
       "      <td>1.269205</td>\n",
       "      <td>0.057657</td>\n",
       "      <td>0.629307</td>\n",
       "      <td>-0.168432</td>\n",
       "      <td>0.443744</td>\n",
       "      <td>0.276539</td>\n",
       "      <td>1.441274</td>\n",
       "      <td>-0.127944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.013114</td>\n",
       "      <td>-0.334412</td>\n",
       "      <td>1.305208</td>\n",
       "      <td>0.837406</td>\n",
       "      <td>-1.126833</td>\n",
       "      <td>-0.064321</td>\n",
       "      <td>-0.594753</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>0.536360</td>\n",
       "      <td>-0.120472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069161</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.112210</td>\n",
       "      <td>-0.016084</td>\n",
       "      <td>0.595033</td>\n",
       "      <td>0.201073</td>\n",
       "      <td>0.278215</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.030762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.969231</td>\n",
       "      <td>-0.233554</td>\n",
       "      <td>0.238473</td>\n",
       "      <td>0.145793</td>\n",
       "      <td>-0.545741</td>\n",
       "      <td>-0.970680</td>\n",
       "      <td>0.347393</td>\n",
       "      <td>-0.209522</td>\n",
       "      <td>-0.342571</td>\n",
       "      <td>-0.100331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240978</td>\n",
       "      <td>-0.362820</td>\n",
       "      <td>-1.417272</td>\n",
       "      <td>0.162136</td>\n",
       "      <td>0.541628</td>\n",
       "      <td>-0.079465</td>\n",
       "      <td>0.268702</td>\n",
       "      <td>-0.101237</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.856523</td>\n",
       "      <td>1.080875</td>\n",
       "      <td>1.866956</td>\n",
       "      <td>1.729941</td>\n",
       "      <td>-0.161741</td>\n",
       "      <td>0.028789</td>\n",
       "      <td>0.401787</td>\n",
       "      <td>0.043774</td>\n",
       "      <td>-0.213916</td>\n",
       "      <td>0.155907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068915</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.077392</td>\n",
       "      <td>-0.221906</td>\n",
       "      <td>0.394141</td>\n",
       "      <td>0.237225</td>\n",
       "      <td>-0.080102</td>\n",
       "      <td>-0.291408</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142398</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142399</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142400</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142401</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142402</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142403 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -0.439952   0.683758  1.225814  0.639113  0.716765  0.089295   \n",
       "1       -4.868108   1.264420 -5.167885  3.193648 -3.045621 -2.096166   \n",
       "2        1.013114  -0.334412  1.305208  0.837406 -1.126833 -0.064321   \n",
       "3        0.969231  -0.233554  0.238473  0.145793 -0.545741 -0.970680   \n",
       "4       -0.856523   1.080875  1.866956  1.729941 -0.161741  0.028789   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "142398 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "142399  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "142400   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "142401  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "142402  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V20       V21  \\\n",
       "0       0.657718  0.034213 -0.472679  0.352698  ...  0.211867  0.127443   \n",
       "1      -6.445610  2.422536 -3.214055 -8.745973  ...  0.667310  1.269205   \n",
       "2      -0.594753  0.147737  0.536360 -0.120472  ...  0.069161  0.017079   \n",
       "3       0.347393 -0.209522 -0.342571 -0.100331  ...  0.240978 -0.362820   \n",
       "4       0.401787  0.043774 -0.213916  0.155907  ... -0.068915  0.007365   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "142398 -4.918215  7.305334  1.914428  4.356170  ...  1.475829  0.213454   \n",
       "142399  0.024330  0.294869  0.584800 -0.975926  ...  0.059616  0.214205   \n",
       "142400 -0.296827  0.708417  0.432454 -0.484782  ...  0.001396  0.232045   \n",
       "142401 -0.686180  0.679145  0.392087 -0.399126  ...  0.127434  0.265245   \n",
       "142402  1.577006 -0.414650  0.486180 -0.915427  ...  0.382948  0.261057   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.588590 -0.145728 -0.322900 -0.299744 -0.310295  0.248317 -0.011493   \n",
       "1       0.057657  0.629307 -0.168432  0.443744  0.276539  1.441274 -0.127944   \n",
       "2       0.112210 -0.016084  0.595033  0.201073  0.278215  0.007457  0.030762   \n",
       "3      -1.417272  0.162136  0.541628 -0.079465  0.268702 -0.101237  0.028234   \n",
       "4       0.077392 -0.221906  0.394141  0.237225 -0.080102 -0.291408  0.092140   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "142398  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n",
       "142399  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n",
       "142400  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n",
       "142401  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n",
       "142402  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "142398      0  \n",
       "142399      0  \n",
       "142400      0  \n",
       "142401      0  \n",
       "142402      0  \n",
       "\n",
       "[142403 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "test_size = test_df['Class'].count()\n",
    "all_true = np.zeros(test_size)\n",
    "all_false = np.ones(test_size)\n",
    "\n",
    "auc_false = roc_auc_score(test_df['Class'], all_false)\n",
    "auc_true = roc_auc_score(test_df['Class'], all_true)\n",
    "\n",
    "acc_false = accuracy_score(test_df['Class'], all_false)\n",
    "acc_true = accuracy_score(test_df['Class'], all_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5 0.0015659782448403474 0.9984340217551596\n"
     ]
    }
   ],
   "source": [
    "print(auc_false, auc_true, acc_false, acc_true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('Class', axis='columns')\n",
    "y_train = train_df['Class']\n",
    "X_test = test_df.drop('Class', axis='columns')\n",
    "y_test = test_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo com alpha=0.0: AUC = 0.9417040358744395\n",
      "Modelo com alpha=0.01: AUC = 0.9102857679932025\n",
      "Modelo com alpha=0.1: AUC = 0.8743972618552748\n",
      "Modelo com alpha=1: AUC = 0.5852017937219731\n",
      "Modelo com alpha=10: AUC = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "alphas = [0.0, 0.01, 0.1, 1, 10]\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Criando o classificador MLP com o valor atual de alpha\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(20, 20, 20, 20, 20),\n",
    "                        activation='relu',\n",
    "                        alpha=alpha,\n",
    "                        random_state=42)\n",
    "\n",
    "    mlp.fit(X_train, y_train)\n",
    "    predictions = mlp.predict(X_test)\n",
    "\n",
    "    # Avaliando o desempenho\n",
    "    accuracy = roc_auc_score(y_test, predictions)\n",
    "    print(f\"Modelo com alpha={alpha}: AUC = {accuracy}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto é possivel observar que a taxa de regularização influencia na medida do resultado, quanto menor o alpha, mais acertos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Questão 3\n",
    "(Primeira análise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.99932586, 0.9996208 , 0.99955759]),\n",
       " array([0.99932586, 0.9994944 , 0.99957865]),\n",
       " array([0.99934693, 0.99964186, 0.99962079])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "layers = [(), (10,), (5, 5)]\n",
    "scores_NN = []\n",
    "\n",
    "for n_layer in layers:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=n_layer,\n",
    "                        random_state=42)\n",
    "    \n",
    "    mlp.fit(X_train, y_train)\n",
    "    # Realizando a validação cruzada\n",
    "    scores_NN.append(cross_val_score(mlp, X_train, y_train, cv=kfold))\n",
    "    \n",
    "scores_NN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.99932586, 0.99945226, 0.99962079]),\n",
       " array([0.99930479, 0.99947333, 0.99964186]),\n",
       " array([0.99930479, 0.99947333, 0.99966292])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "ks = [3, 5, 7]\n",
    "scores_kfold = []\n",
    "\n",
    "for n_k in ks:\n",
    "    knn = KNeighborsClassifier(n_k)\n",
    "    knn.fit(X_test, y_test)\n",
    "    \n",
    "    scores_kfold.append(cross_val_score(knn, X_train, y_train, cv=kfold))\n",
    "    \n",
    "\n",
    "scores_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00025279833702418486"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.subtract(scores_NN, scores_kfold))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Análise com GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros para MLPClassifier:\n",
      "{'hidden_layer_sizes': (10,)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "mlp_parameters = [\n",
    "    {'hidden_layer_sizes': [()]},\n",
    "    {'hidden_layer_sizes': [(10,)]},\n",
    "    {'hidden_layer_sizes': [(5, 5)]}\n",
    "]\n",
    "# Criando o objeto GridSearchCV para cada classificador\n",
    "mlp_grid_search = GridSearchCV(mlp, mlp_parameters, cv=3)\n",
    "\n",
    "\n",
    "# %%time\n",
    "# Realizando a pesquisa em grade para cada classificador\n",
    "mlp_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimindo os melhores hiperparâmetros encontrados para cada classificador\n",
    "print(\"Melhores hiperparâmetros para MLPClassifier:\")\n",
    "print(mlp_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do melhor modelo MLPClassifier: 0.9996488838016053\n"
     ]
    }
   ],
   "source": [
    "# Avaliando o desempenho dos melhores modelos para cada classificador\n",
    "mlp_accuracy = mlp_grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f\"Acurácia do melhor modelo MLPClassifier: {mlp_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros para KNeighborsClassifier:\n",
      "{'n_neighbors': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_parameters = [\n",
    "    {'n_neighbors': [3]},\n",
    "    {'n_neighbors': [5]},\n",
    "    {'n_neighbors': [7]}\n",
    "]\n",
    "\n",
    "knn_grid_search = GridSearchCV(knn, knn_parameters, cv=3)\n",
    "# %%time\n",
    "knn_grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Melhores hiperparâmetros para KNeighborsClassifier:\")\n",
    "print(knn_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do melhor modelo KNeighborsClassifier: 0.999536526618119\n"
     ]
    }
   ],
   "source": [
    "knn_accuracy = knn_grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f\"Acurácia do melhor modelo KNeighborsClassifier: {knn_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001123571834863446"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_accuracy - knn_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A) A técnica de utilizar a rede neural com uma camada intermediária com 10 neuronios se mostrou mais vantajosa comparada ao KNN pois obteve um score 0.00011 maior.\n",
    "* B) O KNN demorou muito mais para gerar as previsões em relação ao MLPClassifier por que o algoritmo precisa calcular a distância dos dados de treinamento para todas as outros dados do conjunto de treinamento. \n",
    "---\n",
    "### Questão 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99878056e-01, 1.21944110e-04],\n",
       "       [1.64489863e-05, 9.99983551e-01],\n",
       "       [9.99155554e-01, 8.44445511e-04],\n",
       "       ...,\n",
       "       [9.99986250e-01, 1.37503010e-05],\n",
       "       [9.99999844e-01, 1.56269857e-07],\n",
       "       [9.99967297e-01, 3.27030049e-05]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = mlp = MLPClassifier(random_state=42,  hidden_layer_sizes=(10,))\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "transactions = best_model.predict_proba(X_test)\n",
    "\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_percent = int(len(transactions) * 0.01)\n",
    "\n",
    "probs_fraud = transactions[:, 1]\n",
    "sorted_indices = np.argsort(probs_fraud)[::-1]\n",
    "sorted_indices = sorted_indices[: one_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se bloquearmos 1% das transações com maiores probabilidades de serem fraudes, teremos 1424 transações bloqueadas,\n",
      "        sendo que 181 tem grande probabilidade de serem fraudes e 1243 não.\n",
      "        baseando-se nisso, é provavel dizer que o  modelo evitaria R$18100 de perdas, e os bloqueios errôneos trariam R$2486 de prejuízo\n"
     ]
    }
   ],
   "source": [
    "frauds = transactions[sorted_indices]\n",
    "\n",
    "not_frauds = [transaction for transaction in frauds if transaction[0] >= transaction[1]]\n",
    "n_true_blocks = len(frauds) - len(not_frauds)\n",
    "\n",
    "gains = 100 * n_true_blocks\n",
    "losses = 2 * len(not_frauds)\n",
    "\n",
    "print(f'''Se bloquearmos 1% das transações com maiores probabilidades de serem fraudes, teremos {len(frauds)} transações bloqueadas,\n",
    "        sendo que {n_true_blocks} tem grande probabilidade de serem fraudes e {len(not_frauds)} não.\n",
    "        baseando-se nisso, é provavel dizer que o  modelo evitaria R${gains} de perdas, e os bloqueios errôneos trariam R${losses} de prejuízo''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Questão 5\n",
    "* O SGDClassifier, armazena os coeficientes atribuídos a cada variável por meio do atributo coef_. Valores próximos de zero indicam que a variável tem menos impacto no modelo. Para isso vamos definir um limiar abitrário de 0.5 em difereça absoluta do 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\holon\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -1.90396652,   3.20133897, -10.07524042,  10.52191773,\n",
       "          3.36484313,  -2.59997983,  -2.87231874,  -3.98241251,\n",
       "         -2.58948718,  -5.88675992,   4.38061579,  -7.4802448 ,\n",
       "         -0.59637301, -12.6140959 ,  -2.27653193,  -3.29237196,\n",
       "         -2.75010288,   0.74038048,  -0.25889901,   0.68420487,\n",
       "          2.23508381,   0.5833491 ,  -0.25864259,   0.10620122,\n",
       "         -0.65497842,  -0.3124277 ,   0.34504393,   0.35544964]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# O hiperparametro loss='log' define a função de ativação como sigmoid\n",
    "sgd_classifier = SGDClassifier(loss='log', random_state=42)\n",
    "\n",
    "sgd_classifier.fit(X_train, y_train)\n",
    "sgd_classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As variáveis não importantes para o problema são: ['V19', 'V23', 'V24', 'V26', 'V27', 'V28']\n"
     ]
    }
   ],
   "source": [
    "below_threshold = np.argwhere(np.abs(sgd_classifier.coef_[0]) < 0.5).reshape((1, -1))\n",
    "\n",
    "below_threshold = [f'V{i+1}' for i in below_threshold[0]]\n",
    "\n",
    "print(f'''As variáveis não importantes para o problema são: {below_threshold}''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
